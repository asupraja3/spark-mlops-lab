{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVPKYVPtwsvsihdkvLIfT1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asupraja3/spark-mlops-lab/blob/main/pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WloSECBXd0cC",
        "outputId": "ff484eaf-fa82-47be-9899-db793b0d6bd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "# Install PySpark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Start the Session and Set Config**\n",
        "* In most environments (like Databricks or pyspark), the spark object is ready. If not, run this:\n"
      ],
      "metadata": {
        "id": "_eo8_aDcktqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# Standard way to get/create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"Pyspark Practice\").getOrCreate()\n",
        "# NEW: Set time policy for reliable date/time parsing across different formats\n",
        "spark.sql(\"SET spark.sql.legacy.timeParserPolicy=LEGACY\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI1jaqukks3N",
        "outputId": "0ac180aa-e8df-4705-f16a-4ce493777bc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[key: string, value: string]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. DataFrame Creation Examples**"
      ],
      "metadata": {
        "id": "tF0iFGt7lrKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#From Python List\n",
        "data = [(\"Alice\", 1, \"2024-01-01\"), (\"Bob\", 2, \"2024-01-02\")]\n",
        "df_data = spark.createDataFrame(data, [\"Name\", \"ID\", \"Date\"])\n",
        "\n",
        "#From JSON File\n",
        "#df_json = spark.read.json(\"/temp.JSON\")\n",
        "\n",
        "#From CSV File\n",
        "#df_json = spark.read.csv(\"/temp.csv\", header=True, inferSchema=True)\n",
        "\n",
        "#From Parquet File\n",
        "#df_json = spark.read.parquet(\"/temp.pq\")"
      ],
      "metadata": {
        "id": "7xKeIMyfmTTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**II. Schema: The Blueprint of Your Data**\n",
        "* A Schema is the definition of the column names and their data types (e.g., String, Integer, Date). Spark can either guess the schema or you can define it precisely.\n",
        "* inferSchema=True: Spark scans the data to guess the types. Quick for small datasets or exploration.\n",
        "* StructType and StructField: Reliable for production ETL. Avoids misinterpretation, like treating a column of numbers as strings.\n",
        "\n"
      ],
      "metadata": {
        "id": "pMui8vOXw-aN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "sample_data = [\n",
        "    (\"Alice\", 25, \"New York\"),\n",
        "    (\"Bob\", 30, \"London\"),\n",
        "    (\"Charlie\", 25, \"New York\")\n",
        "]\n",
        "\n",
        "# 2. Explicitly define the schema (StructType)\n",
        "defined_schema = StructType([StructField(\"name\", StringType(), True),\n",
        "                StructField(\"mployee_age\", IntegerType(), True),\n",
        "                StructField(\"city\", StringType(), True)])\n",
        "\n",
        "# 3. Create the DataFrame using the explicit schema\n",
        "df = spark.createDataFrame(data=sample_data, schema=defined_schema)\n",
        "\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ercE043BxBPE",
        "outputId": "1af74538-b027-4c41-cd15-095d3dec72f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- mployee_age: integer (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            "\n",
            "+-------+-----------+--------+\n",
            "|   name|mployee_age|    city|\n",
            "+-------+-----------+--------+\n",
            "|  Alice|         25|New York|\n",
            "|    Bob|         30|  London|\n",
            "|Charlie|         25|New York|\n",
            "+-------+-----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**III. Core DataFrame Operations**  \n",
        "These are the fundamental ways to manipulate columns and rows.\n",
        "\n",
        "**1. Column Operations (Shape & Content)  **\n",
        "Used to select, add, rename, or remove columns."
      ],
      "metadata": {
        "id": "LunoF8eRo-_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lit\n",
        "# a. SELECT (Selecting and Renaming Columns)\n",
        "df_select = df.select(\"city\", df[\"name\"].alias(\"emp_name\"))\n",
        "df_select.show(3)\n",
        "# b. withColumn (Adding or Modifying a Column)\n",
        "df1 = df.withColumn(\"city_NY\", (df.city == \"New York\"))# Boolean flag\n",
        "df2 = df1.withColumn(\"const\", lit(100)) # Add column with constant value\n",
        "df2.show()\n",
        "# c. drop (Removing Columns)\n",
        "df3 = df2.drop(\"const\")\n",
        "df3.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4x7iLdjx2TL",
        "outputId": "5334b1c0-1161-48aa-e47b-a58c61866023"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+\n",
            "|    city|emp_name|\n",
            "+--------+--------+\n",
            "|New York|   Alice|\n",
            "|  London|     Bob|\n",
            "|New York| Charlie|\n",
            "+--------+--------+\n",
            "\n",
            "+-------+-----------+--------+-------+-----+\n",
            "|   name|mployee_age|    city|city_NY|const|\n",
            "+-------+-----------+--------+-------+-----+\n",
            "|  Alice|         25|New York|   true|  100|\n",
            "|    Bob|         30|  London|  false|  100|\n",
            "|Charlie|         25|New York|   true|  100|\n",
            "+-------+-----------+--------+-------+-----+\n",
            "\n",
            "+-------+-----------+--------+-------+\n",
            "|   name|mployee_age|    city|city_NY|\n",
            "+-------+-----------+--------+-------+\n",
            "|  Alice|         25|New York|   true|\n",
            "|    Bob|         30|  London|  false|\n",
            "|Charlie|         25|New York|   true|\n",
            "+-------+-----------+--------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"name\",\"mployee_age\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5-HlXZzp8sY",
        "outputId": "d0c50b13-df07-4cd6-fa30-1833c94deb22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+\n",
            "|   name|mployee_age|\n",
            "+-------+-----------+\n",
            "|  Alice|         25|\n",
            "|    Bob|         30|\n",
            "|Charlie|         25|\n",
            "+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Filtering Operations (Selecting Rows)**  \n",
        "Used to keep only the rows that match a specified condition. filter() and where() are identical in Spark."
      ],
      "metadata": {
        "id": "2p5pXYEspJjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select rows where age is less than 30\n",
        "df.filter(df.mployee_age < 30).show()\n",
        "\n",
        "# Use 'where' for multi-condition logic (identical to filter)\n",
        "df2 = df.where((df.mployee_age<30) & (df['mployee_age']>=25)) #remeber to seperate with paranthesis\n",
        "df2.show()\n",
        "\n",
        "# isin (Checking if a value is in a list)\n",
        "cities = ['New York']\n",
        "df.filter(df.city.isin(cities)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSufwros2Tmu",
        "outputId": "0070cbe4-e2f7-4eff-f840-09b2b1a2d5a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+--------+\n",
            "|   name|mployee_age|    city|\n",
            "+-------+-----------+--------+\n",
            "|  Alice|         25|New York|\n",
            "|Charlie|         25|New York|\n",
            "+-------+-----------+--------+\n",
            "\n",
            "+-------+-----------+--------+\n",
            "|   name|mployee_age|    city|\n",
            "+-------+-----------+--------+\n",
            "|  Alice|         25|New York|\n",
            "|Charlie|         25|New York|\n",
            "+-------+-----------+--------+\n",
            "\n",
            "+-------+-----------+--------+\n",
            "|   name|mployee_age|    city|\n",
            "+-------+-----------+--------+\n",
            "|  Alice|         25|New York|\n",
            "|Charlie|         25|New York|\n",
            "+-------+-----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Sorting Operations (Ordering Rows)**\n",
        "Used to order the final result set."
      ],
      "metadata": {
        "id": "BmxZeYXdr9Ig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# a. orderBy (Primary method for sorting)\n",
        "df.orderBy(col(\"city\").asc(), col('mployee_age').desc()).show() #notice asc is a function\n",
        "# b. sort (Alias for orderBy)\n",
        "df.sort(\"name\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-At_bn50sAcH",
        "outputId": "fe6503e2-0eaf-46f9-cfdf-e5d59d8ed214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+--------+\n",
            "|   name|mployee_age|    city|\n",
            "+-------+-----------+--------+\n",
            "|    Bob|         30|  London|\n",
            "|  Alice|         25|New York|\n",
            "|Charlie|         25|New York|\n",
            "+-------+-----------+--------+\n",
            "\n",
            "+-------+-----------+--------+\n",
            "|   name|mployee_age|    city|\n",
            "+-------+-----------+--------+\n",
            "|  Alice|         25|New York|\n",
            "|    Bob|         30|  London|\n",
            "|Charlie|         25|New York|\n",
            "+-------+-----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.sort(df.name.asc()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0gd4lJhtYkS",
        "outputId": "7483fca6-3b76-4a1a-8245-d22f142683a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+--------+\n",
            "|   name|mployee_age|    city|\n",
            "+-------+-----------+--------+\n",
            "|  Alice|         25|New York|\n",
            "|    Bob|         30|  London|\n",
            "|Charlie|         25|New York|\n",
            "+-------+-----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I. Aggregations (Summarizing Data) 📊**  \n",
        "Aggregations summarize data across groups.  \n",
        "**groupBy:**\tGroups rows based on one or more columns (e.g., grouping sales by region).  \n",
        "**agg:**\tApplies summary functions (like sum, avg, count) to the grouped data.  \n",
        "**pivot:**\tRotates a unique value from one column into multiple new columns (like creating a cross-tabulation table).  \n",
        "**rollup:** \tCreates subtotals for hierarchical columns. (e.g., Total Sales for Year, then for Quarter within Year, then a Grand Total.)  \n",
        "**cube:**\tCreates subtotals for every possible combination of grouping columns, regardless of hierarchy. (e.g., Sales by Region, by Product, and by Region-Product combination.)"
      ],
      "metadata": {
        "id": "Xrm0gkK9uYlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum, avg, count\n",
        "\n",
        "# Sample DataFrame (DF)\n",
        "data = [(\"A\", \"X\", 10), (\"A\", \"Y\", 20), (\"B\", \"X\", 15), (\"B\", \"Y\", 5)]\n",
        "df = spark.createDataFrame(data, [\"key1\", \"key2\", \"value\"])\n",
        "\n",
        "# Simple GroupBy and Agg\n",
        "df.groupBy('key1').agg(count('*').alias(\"total count\"), sum('value').alias('total value')).show()\n",
        "\n",
        "# Pivot Example (Move Key2 values into columns)\n",
        "df.groupBy('key1').pivot('key2').agg(sum('value')).show()\n",
        "\n",
        "# Rollup Example\n",
        "df.rollup('key1','key2').agg(sum('value')).sort(\"key1\", \"key2\").show()\n",
        "\n",
        "# Cube Example\n",
        "df.cube('key1','key2').agg(sum('value')).sort(\"key1\", \"key2\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FNyeAF6wFub",
        "outputId": "31440af1-e8d3-454e-bf89-791afdc2f49a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----------+-----------+\n",
            "|key1|total count|total value|\n",
            "+----+-----------+-----------+\n",
            "|   A|          2|         30|\n",
            "|   B|          2|         20|\n",
            "+----+-----------+-----------+\n",
            "\n",
            "+----+---+---+\n",
            "|key1|  X|  Y|\n",
            "+----+---+---+\n",
            "|   B| 15|  5|\n",
            "|   A| 10| 20|\n",
            "+----+---+---+\n",
            "\n",
            "+----+----+----------+\n",
            "|key1|key2|sum(value)|\n",
            "+----+----+----------+\n",
            "|NULL|NULL|        50|\n",
            "|   A|NULL|        30|\n",
            "|   A|   X|        10|\n",
            "|   A|   Y|        20|\n",
            "|   B|NULL|        20|\n",
            "|   B|   X|        15|\n",
            "|   B|   Y|         5|\n",
            "+----+----+----------+\n",
            "\n",
            "+----+----+----------+\n",
            "|key1|key2|sum(value)|\n",
            "+----+----+----------+\n",
            "|NULL|NULL|        50|\n",
            "|NULL|   X|        25|\n",
            "|NULL|   Y|        25|\n",
            "|   A|NULL|        30|\n",
            "|   A|   X|        10|\n",
            "|   A|   Y|        20|\n",
            "|   B|NULL|        20|\n",
            "|   B|   X|        15|\n",
            "|   B|   Y|         5|\n",
            "+----+----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**II. Joins (Combining Data) 🔗**  \n",
        "Joins combine rows from two DataFrames based on a shared key.  \n",
        "### Spark Join Types\n",
        "\n",
        "- **Type: inner**  \n",
        "  **What it Does:** Keeps only rows that have matches in both DataFrames. (Default.)  \n",
        "  **Result Size:** Smallest  \n",
        "\n",
        "- **Type: left**  \n",
        "  **What it Does:** Keeps all rows from the left DF; includes matching rows from the right (filling with nulls if no match).  \n",
        "  **Result Size:** Size of Left DF (or larger)  \n",
        "\n",
        "- **Type: right**  \n",
        "  **What it Does:** Keeps all rows from the right DF; includes matching rows from the left.  \n",
        "  **Result Size:** Size of Right DF (or larger)  \n",
        "\n",
        "- **Type: outer**  \n",
        "  **What it Does:** Keeps all rows from both DFs; fills missing columns with nulls.  \n",
        "  **Result Size:** Largest  \n",
        "\n",
        "- **Type: left_semi**  \n",
        "  **What it Does:** Keeps only the matching rows from the left DF. Does not include any columns from the right DF.  \n",
        "  **Result Size:** Size of Left DF (or smaller)  \n",
        "\n",
        "- **Type: left_anti**  \n",
        "  **What it Does:** Keeps only the rows from the left DF that do not have a match in the right DF (The unmatched rows).  \n",
        "  **Result Size:** Size of Left DF (or smaller)  \n"
      ],
      "metadata": {
        "id": "1ioErsl9zeWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Self-Joins ➕**  \n",
        "A Self-Join is simply a standard join (usually inner or left) where you join a DataFrame to itself. You do this to compare rows within the same table, often using aliases to distinguish the two sides.\n",
        "\n",
        "*Example Use:* Finding employees who report to the same manager, or finding pairs of products with the same price."
      ],
      "metadata": {
        "id": "OO_uPkqm0gd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample DataFrames\n",
        "employee_data = [(\"Alice\", 101, 201), (\"Bob\", 102, 201), (\"Charlie\", 103, 202), (\"David\", 104, None)]\n",
        "dept_data = [(201, \"Sales\"), (203, \"Marketing\")]\n",
        "\n",
        "df_emp = spark.createDataFrame(employee_data, [\"Name\", \"EmpID\", \"DeptID\"])\n",
        "df_dept = spark.createDataFrame(dept_data, [\"DeptID\", \"DeptName\"])\n",
        "\n",
        "# Inner Join\n",
        "df_emp.join(df_dept, 'DeptID', 'inner').show()\n",
        "# Left Anti Join (Employees without a matching department)\n",
        "df_emp.join(df_dept, 'DeptID', 'left_anti').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M2cM6gH0da-",
        "outputId": "d7687abe-b88f-491a-95eb-86615cf9dd80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-----+--------+\n",
            "|DeptID| Name|EmpID|DeptName|\n",
            "+------+-----+-----+--------+\n",
            "|   201|Alice|  101|   Sales|\n",
            "|   201|  Bob|  102|   Sales|\n",
            "+------+-----+-----+--------+\n",
            "\n",
            "+------+-------+-----+\n",
            "|DeptID|   Name|EmpID|\n",
            "+------+-------+-----+\n",
            "|   202|Charlie|  103|\n",
            "|  NULL|  David|  104|\n",
            "+------+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Self-Join Example\n",
        "df_mgr = df_emp.alias(\"Manager\")\n",
        "df_emp_join = df_emp.alias(\"Employee\")\n",
        "\n",
        "df_self_join = df_emp_join.join(\n",
        "    df_mgr,\n",
        "    df_emp_join.DeptID == df_mgr.DeptID,\n",
        "    \"inner\"\n",
        ").select(\n",
        "    df_emp_join.Name.alias(\"Employee\"),\n",
        "    df_mgr.Name.alias(\"Peer\")\n",
        ").filter(\"Employee != Peer\") # Remove self-matches\n",
        "print(\"--- Self Join (Employees with the same DeptID/Peer) ---\")\n",
        "df_self_join.show()"
      ],
      "metadata": {
        "id": "02pWU29r5m3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Spark SQL 📝**  \n",
        "Spark SQL allows you to use familiar SQL syntax directly on your DataFrames.\n",
        "\n",
        "**createOrReplaceTempView(name):** This action makes a DataFrame available as a temporary SQL table (a \"view\") for the current SparkSession. It's the bridge between the DataFrame API and the SQL API.\n",
        "\n",
        "**spark.sql(\"...\"):** This executes a SQL query string against the defined views and returns the result as a new DataFrame."
      ],
      "metadata": {
        "id": "aT6HovlN6GCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a temporary view from the employee DataFrame\n",
        "df_emp.createOrReplaceTempView('Employee')\n",
        "\n",
        "# Run a SQL query against the view\n",
        "spark.sql(\"SELECT Name FROM Employee WHERE EmpID = '103'\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vs0T50446NFG",
        "outputId": "97382c90-75e6-4957-8a86-0b92fc6c8bbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|   Name|\n",
            "+-------+\n",
            "|Charlie|\n",
            "+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Caching Strategies 💾**\n",
        "\n",
        "Caching is how Spark achieves its high speed and fault tolerance: it saves the intermediate results of an RDD/DataFrame in memory or on disk for faster reuse.\n"
      ],
      "metadata": {
        "id": "mnO1FloN5r5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.storagelevel import StorageLevel\n",
        "\n",
        "# Cache the employee DataFrame using the default (MEMORY_ONLY)\n",
        "df_emp.cache()\n",
        "# Alternative: df_emp.persist(StorageLevel.MEMORY_AND_DISK)\n",
        "\n",
        "# Force Spark to materialize and store the data now\n",
        "df_emp.count()\n",
        "\n",
        "print(\"DataFrame 'df_emp' is now cached in memory.\")"
      ],
      "metadata": {
        "id": "hDcNDI9C8jb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Window Functions (rank, dense_rank, lead, lag, rowsBetween, rangeBetween)**"
      ],
      "metadata": {
        "id": "x-EqVtS8LmRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank, dense_rank, lag, sum, desc, col\n",
        "\n",
        "# df created with: (\"A\", 200, 2), (\"A\", 100, 1), (\"A\", 100, 3), (\"B\", 150, 5), (\"B\", 50, 4)\n",
        "df = spark.createDataFrame([\n",
        "    (\"A\", 200, 1), (\"A\", 200, 2), (\"A\", 100, 3), (\"A\", 300, 4), (\"B\", 50, 4), (\"B\", 150, 5)\n",
        "], [\"category\", \"sales\", \"day\"])\n",
        "\n",
        "# Ranking window: Partition by category, ordered by sales descending\n",
        "rank_window = Window.partitionBy(\"category\").orderBy(desc(\"sales\"))\n",
        "\n",
        "# Time-series/Rolling window: Partition by category, ordered by day\n",
        "ts_window = Window.partitionBy(\"category\").orderBy(\"day\")\n",
        "\n",
        "# Rolling sum: look at 2 rows before and including the current row\n",
        "rolling_window = ts_window.rowsBetween(-2, 0)\n",
        "\n",
        "result = df.withColumn(\"rank\", rank().over(rank_window)) \\\n",
        "           .withColumn(\"dense_rank\", dense_rank().over(rank_window)) \\\n",
        "           .withColumn(\"prev_sales\", lag(col(\"sales\"), 1).over(ts_window)) \\\n",
        "           .withColumn(\"roll_sum_sales\", sum(\"sales\").over(rolling_window))\n",
        "\n",
        "print(\"--- Window Functions Output ---\")\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTl7SucsLmr5",
        "outputId": "26b7f4ac-2f09-4820-eb23-d0731aeed556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Window Functions Output ---\n",
            "+--------+-----+---+----+----------+----------+--------------+\n",
            "|category|sales|day|rank|dense_rank|prev_sales|roll_sum_sales|\n",
            "+--------+-----+---+----+----------+----------+--------------+\n",
            "|       A|  200|  1|   2|         2|      NULL|           200|\n",
            "|       A|  200|  2|   2|         2|       200|           400|\n",
            "|       A|  100|  3|   4|         3|       200|           500|\n",
            "|       A|  300|  4|   1|         1|       100|           600|\n",
            "|       B|   50|  4|   2|         2|      NULL|            50|\n",
            "|       B|  150|  5|   1|         1|        50|           200|\n",
            "+--------+-----+---+----+----------+----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's create a sample DataFrame representing a few credit trades made by different traders during the day."
      ],
      "metadata": {
        "id": "0W4ZB8pjC60M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, Window\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"CitadelWindowFunctions\").getOrCreate()\n",
        "\n",
        "# Sample trade data for Credit Default Swaps (CDS)\n",
        "trade_data = [\n",
        "    (\"trader_A\", \"2025-10-06 09:01:00\", 5000000),\n",
        "    (\"trader_B\", \"2025-10-06 09:02:00\", 7000000),\n",
        "    (\"trader_A\", \"2025-10-06 09:03:00\", 8000000), # Trader A's biggest trade\n",
        "    (\"trader_C\", \"2025-10-06 09:04:00\", 6000000),\n",
        "    (\"trader_B\", \"2025-10-06 09:05:00\", 7000000), # Tied with their first trade\n",
        "    (\"trader_A\", \"2025-10-06 09:06:00\", 2000000),\n",
        "]\n",
        "columns = [\"trader_id\", \"trade_time\", \"notional_usd\"]\n",
        "trades_df = spark.createDataFrame(trade_data, columns)"
      ],
      "metadata": {
        "id": "bUzQ25ozCOG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Ranking Functions**:   \n",
        "rank(), dense_rank(), row_number()\n",
        "These functions assign a rank to each row within a partition based on some ordering.\n",
        "\n",
        "*Real-time Scenario:* A trading desk manager at Citadel wants to quickly identify the top 3 largest trades (by notional value) for each trader to review their daily performance and risk exposure."
      ],
      "metadata": {
        "id": "vCQPxl22Chvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a window partitioned by trader and ordered by trade size (descending)\n",
        "window_spec_rank = Window.partitionBy(\"trader_id\").orderBy(desc(\"notional_usd\"))\n",
        "# Apply ranking functions\n",
        "df1 = trades_df.withColumn(\"rank\", F.rank().over(window_spec_rank))\\\n",
        "              .withColumn(\"dense_rank\", F.dense_rank().over(window_spec_rank)\\\n",
        "              .withColumn(\"row_num\", F.row_number().over(window_spec_rank)\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpqaQTKxCT1J",
        "outputId": "20d38b8e-c564-4b80-dd7c-5f24ab05c4d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------------+------------+----+----------+-------+\n",
            "|trader_id|         trade_time|notional_usd|rank|dense_rank|row_num|\n",
            "+---------+-------------------+------------+----+----------+-------+\n",
            "| trader_A|2025-10-06 09:03:00|     8000000|   1|         1|      1|\n",
            "| trader_A|2025-10-06 09:01:00|     5000000|   2|         2|      2|\n",
            "| trader_A|2025-10-06 09:06:00|     2000000|   3|         3|      3|\n",
            "| trader_B|2025-10-06 09:02:00|     7000000|   1|         1|      1|\n",
            "| trader_B|2025-10-06 09:05:00|     7000000|   1|         1|      2|\n",
            "| trader_C|2025-10-06 09:04:00|     6000000|   1|         1|      1|\n",
            "+---------+-------------------+------------+----+----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Analytic Functions: lag() and lead()**\n",
        "\n",
        "These functions access data from a previous (lag) or subsequent (lead) row within the same partition.\n",
        "\n",
        "**Real-time Scenario:** For a high-frequency credit trading algorithm, you need to calculate the change in notional value between a trader's consecutive trades. This helps in analyzing trading patterns or identifying sudden, large changes in position size that might trigger a risk alert."
      ],
      "metadata": {
        "id": "ENvYiOG7C1C9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a window partitioned by trader and ordered by time\n",
        "window_spec_time = Window.partitionBy(\"trader_id\").orderBy(\"trade_time\")\n",
        "# Apply ranking functions\n",
        "df3 = trades_df.withColumn(\"lag\", F.lag(\"notional_usd\", 1).over(window_spec_time))\n",
        "df3.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ssp3mL_WDAuC",
        "outputId": "c5db579c-e531-4d61-d4c5-81ec2bff3c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------------+------------+-------+\n",
            "|trader_id|         trade_time|notional_usd|    lag|\n",
            "+---------+-------------------+------------+-------+\n",
            "| trader_A|2025-10-06 09:01:00|     5000000|   NULL|\n",
            "| trader_A|2025-10-06 09:03:00|     8000000|5000000|\n",
            "| trader_A|2025-10-06 09:06:00|     2000000|8000000|\n",
            "| trader_B|2025-10-06 09:02:00|     7000000|   NULL|\n",
            "| trader_B|2025-10-06 09:05:00|     7000000|7000000|\n",
            "| trader_C|2025-10-06 09:04:00|     6000000|   NULL|\n",
            "+---------+-------------------+------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Aggregate Functions Over a Window: sum()**  \n",
        "You can use standard aggregate functions like sum(), avg(), max(), etc., over a window to create running totals or moving averages.\n",
        "\n",
        "**Real-time Scenario:** A risk management system at Citadel needs to calculate the cumulative trading volume (running total) for each trader throughout the day. This is critical for monitoring intraday credit limits and ensuring no single trader exceeds their authorized exposure."
      ],
      "metadata": {
        "id": "8sN904U9DC_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a window that includes all rows from the start up to the current row for each trader\n",
        "window_spec_running_total = Window.partitionBy(\"trader_id\").orderBy(\"trade_time\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "\n",
        "# Calculate the running total of notional value\n",
        "df4 = trades_df.withColumn(\"sum\", F.sum(\"notional_usd\").over(window_spec_running_total))\n",
        "df4.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMHaKQwXDKs0",
        "outputId": "bf271bec-527e-400d-a18c-aa55e961d411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------------+------------+--------+\n",
            "|trader_id|         trade_time|notional_usd|     sum|\n",
            "+---------+-------------------+------------+--------+\n",
            "| trader_A|2025-10-06 09:01:00|     5000000| 5000000|\n",
            "| trader_A|2025-10-06 09:03:00|     8000000|13000000|\n",
            "| trader_A|2025-10-06 09:06:00|     2000000|15000000|\n",
            "| trader_B|2025-10-06 09:02:00|     7000000| 7000000|\n",
            "| trader_B|2025-10-06 09:05:00|     7000000|14000000|\n",
            "| trader_C|2025-10-06 09:04:00|     6000000| 6000000|\n",
            "+---------+-------------------+------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Python UDF/Pandas UDF**"
      ],
      "metadata": {
        "id": "gof4_q3GsOrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql.functions import udf, pandas_udf, col\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "df = spark.createDataFrame([(10,), (20,), (30,)], [\"feature_a\"])\n",
        "\n",
        "# Standard Python UDF (Slow: Row-at-a-time)\n",
        "def python_udf_logic(a):\n",
        "    return a * 1.5 + 2\n",
        "\n",
        "python_udf = udf(python_udf_logic, DoubleType())\n",
        "\n",
        "# Pandas UDF (Fast: Batch/Vectorized processing via Arrow)\n",
        "@pandas_udf(DoubleType())\n",
        "def pandas_udf_logic(a: pd.Series) -> pd.Series:\n",
        "    # Vectorized operation is done here\n",
        "    return a * 1.5 + 2\n",
        "\n",
        "print(\"--- UDF Comparison Output ---\")\n",
        "result = df.withColumn(\"python_udf_result\", python_udf(col(\"feature_a\"))) \\\n",
        "           .withColumn(\"pandas_udf_result\", pandas_udf_logic(col(\"feature_a\")))\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPp6ieOqsZ-F",
        "outputId": "79c20804-5024-46f3-e688-b0a1d0e2cb27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- UDF Comparison Output ---\n",
            "+---------+-----------------+-----------------+\n",
            "|feature_a|python_udf_result|pandas_udf_result|\n",
            "+---------+-----------------+-----------------+\n",
            "|       10|             17.0|             17.0|\n",
            "|       20|             32.0|             32.0|\n",
            "|       30|             47.0|             47.0|\n",
            "+---------+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Complex Types (arrays/maps/structs) with explode, posexplode**"
      ],
      "metadata": {
        "id": "DMqqDEK0vZDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, Window\n",
        "from pyspark.sql.functions import explode, posexplode, concat, lit, row_number, count\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName(\"pyspark1\").getOrCreate()\n",
        "\n",
        "data = [(\"User1\", [\"tag1\", \"tag2\", \"tag3\"]),\n",
        "        (\"User2\", [\"tagA\"])]\n",
        "column_name = [\"user\", \"tags\"]\n",
        "df = spark.createDataFrame(data,column_name)\n",
        "\n",
        "df1 = df.select(df.user, explode(df.tags).alias('Tag'))\n",
        "df1.show()\n",
        "\n",
        "df2 = df.select(df.user, posexplode(df.tags))\\\n",
        "                   .withColumnRenamed(\"pos\", \"tag_index\") \\\n",
        "                   .withColumnRenamed(\"col\", \"single_tag\")\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PS48_PbEvcua",
        "outputId": "cc08370e-2d7f-4f8d-b9a5-54e493e54713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+\n",
            "| user| Tag|\n",
            "+-----+----+\n",
            "|User1|tag1|\n",
            "|User1|tag2|\n",
            "|User1|tag3|\n",
            "|User2|tagA|\n",
            "+-----+----+\n",
            "\n",
            "+-----+---------+----------+\n",
            "| user|tag_index|single_tag|\n",
            "+-----+---------+----------+\n",
            "|User1|        0|      tag1|\n",
            "|User1|        1|      tag2|\n",
            "|User1|        2|      tag3|\n",
            "|User2|        0|      tagA|\n",
            "+-----+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df3 = df1.withColumn(\"litcol\", concat(df1.user,lit(i)) for i in range(1,4))\n",
        "window1 = Window().orderBy(df1.user)\n",
        "df3 = df1.withColumn(\"litcol\", concat(df1.user,lit(row_number().over(window1))))\n",
        "# df3 = df1.withColumn(\"litcol\", concat(lit(\"user\"),monotonically_increasing_id()))\n",
        "df3.show()\n",
        "df4 = df1.withColumn(\"litcol\", concat(df1.user,row_number().over(window1)))\n",
        "df4.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCYmo8UkBnbV",
        "outputId": "c0406070-55c9-47c5-ab1c-87c4c35e3a3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+------+\n",
            "| user| Tag|litcol|\n",
            "+-----+----+------+\n",
            "|User1|tag1|User11|\n",
            "|User1|tag2|User12|\n",
            "|User1|tag3|User13|\n",
            "|User2|tagA|User24|\n",
            "+-----+----+------+\n",
            "\n",
            "+-----+----+------+\n",
            "| user| Tag|litcol|\n",
            "+-----+----+------+\n",
            "|User1|tag1|User11|\n",
            "|User1|tag2|User12|\n",
            "|User1|tag3|User13|\n",
            "|User2|tagA|User24|\n",
            "+-----+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Caching in Pyspark**"
      ],
      "metadata": {
        "id": "Mjzq3f7pnhoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CRITICAL STEP: Cache the result of the expensive work ---\n",
        "processed_features.cache()\n",
        "\n",
        "print(\"--- Running Scenario B: WITH CACHING ---\")\n",
        "start_time = time.time()\n",
        "\n",
        "for i in range(ITERATIONS):\n",
        "    # Action: The first count() triggers the caching.\n",
        "    # Subsequent count() calls read from the cache.\n",
        "    count = processed_features.count()\n",
        "    print(f\"Iteration {i+1} (Cached): Counted {count} records.\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Total time WITH Caching: {end_time - start_time:.4f} seconds.\")\n",
        "\n",
        "# Clean up memory after the iterative process is done\n",
        "processed_features.unpersist()\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "nnifCg_ynhWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pytest and SparkTestingBase in the Code**"
      ],
      "metadata": {
        "id": "vcPrn64w_SK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A. Isolating Transformation Logic (The Code)**"
      ],
      "metadata": {
        "id": "5d7gfzxq_j4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import DataFrame, functions as F\n",
        "\n",
        "# This is the core 'unit' of business logic you must test\n",
        "def calculate_spread_change_signal(df: DataFrame) -> DataFrame:\n",
        "    \"\"\"Calculates the 5-day change in the CDS spread and flags a trade signal.\"\"\"\n",
        "\n",
        "    # 1. Feature Engineering (The business logic)\n",
        "    df = df.withColumn(\n",
        "        \"prev_5d_spread\",\n",
        "        F.lag(F.col(\"cds_spread\"), 5).over(Window.partitionBy(\"ticker\").orderBy(\"trade_date\"))\n",
        "    )\n",
        "\n",
        "    # 2. Transformation\n",
        "    df = df.withColumn(\n",
        "        \"spread_change\",\n",
        "        F.col(\"cds_spread\") - F.col(\"prev_5d_spread\")\n",
        "    )\n",
        "\n",
        "    # 3. Signal Generation\n",
        "    df = df.withColumn(\n",
        "        \"is_buy_signal\",\n",
        "        F.when(F.col(\"spread_change\") < -0.05, 1).otherwise(0) # Logic: Buy if spread tightens significantly\n",
        "    )\n",
        "    return df.drop(\"prev_5d_spread\")"
      ],
      "metadata": {
        "id": "2O--i-JM_bgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**B. The Pytest/Testing Code**"
      ],
      "metadata": {
        "id": "E7ufyV2Q_rB6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: The Spark Fixture (Setup)  \n",
        "You use a pytest fixture (usually in a file named conftest.py) to create a single, lightweight local Spark Session that is reused across hundreds of tests, eliminating startup overhead."
      ],
      "metadata": {
        "id": "3TrWRYhG_4nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# conftest.py\n",
        "import pytest\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "@pytest.fixture(scope=\"session\")\n",
        "def spark_session():\n",
        "    # Setup Spark in local mode for fast, single-machine testing\n",
        "    spark = SparkSession.builder.master(\"local[*]\").appName(\"test_session\").getOrCreate()\n",
        "    yield spark # Provide the session to tests\n",
        "    spark.stop() # Teardown after all tests finish"
      ],
      "metadata": {
        "id": "SLaFiHlk_yBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: The Unit Test (Act & Assert)\n",
        "\n",
        "A test function is written that uses the spark_session fixture and compares the output DataFrame against a manually defined expected DataFrame."
      ],
      "metadata": {
        "id": "_MsY6Dn5_-7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test_credit_logic.py\n",
        "from your_module import calculate_spread_change_signal\n",
        "# Assuming you use spark-testing-base or chispa for comparison logic\n",
        "\n",
        "def test_spread_tightening_triggers_buy_signal(spark_session):\n",
        "    # ARRANGE: Create Canned Input Data (The \"Unit\" Input)\n",
        "    input_data = [\n",
        "        (\"CDE\", \"2025-10-01\", 1.50),\n",
        "        (\"CDE\", \"2025-10-02\", 1.45),\n",
        "        (\"CDE\", \"2025-10-03\", 1.40),\n",
        "        (\"CDE\", \"2025-10-04\", 1.35),\n",
        "        (\"CDE\", \"2025-10-05\", 1.20)  # Spread tightens by 0.30\n",
        "    ]\n",
        "    input_df = spark_session.createDataFrame(input_data, [\"ticker\", \"trade_date\", \"cds_spread\"])\n",
        "\n",
        "    # ACT: Run the function under test\n",
        "    actual_df = calculate_spread_change_signal(input_df)\n",
        "\n",
        "    # ASSERT: Define Expected Output Data\n",
        "    expected_data = [\n",
        "        (\"CDE\", \"2025-10-01\", 1.50, None, None),\n",
        "        # ... rows 2, 3, 4 will have change < -0.05, so signal=0\n",
        "        (\"CDE\", \"2025-10-05\", 1.20, -0.30, 1) # Signal=1 because 1.20 - 1.50 = -0.30 < -0.05\n",
        "    ]\n",
        "    expected_df = spark_session.createDataFrame(expected_data, [\"ticker\", \"trade_date\", \"cds_spread\", \"spread_change\", \"is_buy_signal\"])\n",
        "\n",
        "    # Final ASSERTION using a DataFrame comparison utility\n",
        "    # spark_testing_base or similar utility handles the comparison logic (order, schema, data)\n",
        "    assert_dfs_equal(actual_df.sort(\"trade_date\"), expected_df.sort(\"trade_date\"))"
      ],
      "metadata": {
        "id": "zOqw2rLnADSm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}